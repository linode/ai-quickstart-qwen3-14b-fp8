services:

  #--------------------------------------------
  # Caddy - Reverse Proxy with Auto HTTPS
  #--------------------------------------------
  caddy:
    image: caddy:2-alpine
    container_name: caddy
    restart: unless-stopped
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./Caddyfile:/etc/caddy/Caddyfile
      - caddy_data:/data
      - caddy_config:/config
    depends_on:
      - open-webui

  vllm:
    image: vllm/vllm-openai:latest
    container_name: vllm
    restart: unless-stopped
    shm_size: 8g
    ports:
      - "127.0.0.1:8000:8000"
    volumes:
      - vllm_data:/root/.cache/huggingface
      - ./configs:/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/quantization/utils/configs
    environment:
      - HF_HOME=/root/.cache/huggingface
    command:
      - Qwen/Qwen3-14B-FP8
      - --kv-cache-dtype=fp8
      - --gpu-memory-utilization=0.95
      - --max-model-len=16384
      - --max-num-seqs=1
      - --enable-prompt-tokens-details
      # qwen3 supported max context length is 32k
      # for 20GB VRAM with fp8 kv-cache, 16k context length is close to VRAM limit
      # if you like more max num seqs, reduce max model len accordingly
    healthcheck:
      test: ["CMD", "curl", "-f", "-s", "http://localhost:8000/v1/models"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 180s
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  #--------------------------------------------
  # Open-WebUI
  #--------------------------------------------
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    restart: unless-stopped
    ports:
      - "127.0.0.1:8080:8080"
    volumes:
      - open_webui_data:/app/backend/data
    environment:
      - OPENAI_API_BASE_URL=http://vllm:8000/v1
      - OPENAI_API_KEY=none
      - ANONYMIZED_TELEMETRY=false

volumes:
  caddy_data:
  caddy_config:
  vllm_data:
  open_webui_data:
